{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from onlineGreedyfile import onlineGreedy \n",
    "from onlinePrototypes import protonline\n",
    "from onlineCriticism import crionline\n",
    "\n",
    "import math\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import sparse \n",
    "\n",
    "import sys\n",
    "import time\n",
    "\n",
    "from kernelgenerator import compute_kernel\n",
    "\n",
    "sys.path.insert(0, './src')\n",
    "sys.path.insert(0, 'rff/ext')\n",
    "import subsample, herding, rptree, rff\n",
    "\n",
    "sys.path.insert(0, 'MMD')\n",
    "from data import load_svmlight_file\n",
    "from data import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting rff/ext/MNIST/train-images-idx3-ubyte.gz\n",
      "Extracting rff/ext/MNIST/train-labels-idx1-ubyte.gz\n",
      "Extracting rff/ext/MNIST/t10k-images-idx3-ubyte.gz\n",
      "Extracting rff/ext/MNIST/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Download and read MNIST data\n",
    "import input_data\n",
    "mnist = input_data.read_data_sets('rff/ext/MNIST')\n",
    "M = 50\n",
    "N = 50000\n",
    "stream = mnist.train.images[:N]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SyntheticTarget(object):\n",
    "    \n",
    "    def __init__(self, dist_components=2, dim=2, seed=None):\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "        self.dist_weights = stats.dirichlet(np.ones(dist_components)*10).rvs().ravel()\n",
    "        dist_centers = np.random.uniform(-4, 4, size=(dist_components, dim))\n",
    "\n",
    "        def gen_cov():\n",
    "            assert(dim == 2)\n",
    "            rho = np.random.rand()-0.5\n",
    "            sd = np.random.rand(dim)*0.4+0.5\n",
    "            L = np.zeros((dim,dim))\n",
    "            L[0,0] = sd[0]**2\n",
    "            L[1,1] = sd[1]**2\n",
    "            L[0,1] = rho*sd.prod()\n",
    "            return np.dot(L, L.T)\n",
    "\n",
    "        dist_cov = np.array([gen_cov() for i in xrange(dist_components)])\n",
    "        self.dist_obj = np.array([stats.multivariate_normal(dist_centers[k], dist_cov[k]) for k in xrange(dist_components)], dtype=object)\n",
    "        self.dim = dim\n",
    "\n",
    "    def compute_pdf(self, x):\n",
    "        return np.sum(self.dist_weights[:,None,None] * np.array([d.pdf(x) for d in self.dist_obj]), 0)\n",
    "\n",
    "    def draw_sample(self, ns=1):\n",
    "        component = np.random.multinomial(ns, self.dist_weights) # np.argmin(np.random.rand() > self.dist_weights.cumsum())\n",
    "        offset = np.concatenate(([0], component.cumsum()[:-1]))\n",
    "        samples = np.empty((ns,self.dim))\n",
    "        for c, n in enumerate(component):\n",
    "            if n > 0:\n",
    "                samples[offset[c]:offset[c]+n] = self.dist_obj[c].rvs(n) \n",
    "        np.random.shuffle(samples)\n",
    "        return samples\n",
    "\n",
    "    def draw_sample_slow(self):\n",
    "        component = np.argmin(np.random.rand() > self.dist_weights.cumsum())\n",
    "        return self.dist_obj[component].rvs()\n",
    "\n",
    "    def plot(self):\n",
    "        delta = 0.1\n",
    "        boundary = 5\n",
    "        x = np.arange(-boundary, boundary, delta)\n",
    "        y = np.arange(-boundary, boundary, delta)\n",
    "        X, Y = np.meshgrid(x, y)\n",
    "        Z = np.log(self.compute_pdf(np.concatenate((X[:,:,None],Y[:,:,None]),2)))\n",
    "        plt.figure()\n",
    "        contour_levels = np.arange(-9,1)\n",
    "        CS = plt.contour(X, Y, Z, levels=contour_levels)\n",
    "        plt.set_cmap('Blues')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division  #So that when we divide by integers we gegt a float. Eg so that 5/2 = 2.5 not 2.\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def online_loss_reg_max(stream, gamma, new_phi, phis_for_criticisms, full_phi_sum, proto_phi_sum, prototype_indices, criticism_indices, n, M, full_stream_phi, N):\n",
    "    #ONLINE_LOSS_MAX maximizes L(C) online.\n",
    "    #Here we are adding x_n and assume that stored_phis and full_phi_sum have been updated to include x_new: the new data point.\n",
    "    \n",
    "    criticism_indices = np.hstack((criticism_indices,n-1))\n",
    "    criticism_kernel_matrix = compute_kernel(stream[criticism_indices], M+1, gamma)\n",
    "    phis_for_criticisms = np.vstack((phis_for_criticisms,new_phi))\n",
    "    #print phis_for_criticisms.shape\n",
    "    \n",
    "    losses = np.zeros(M+1)\n",
    "    overall_losses = np.zeros(M+1)\n",
    "    for l in range(M+1):\n",
    "        #losses[l] = np.abs((1/n)*sum(kernel_matrix[range(n),criticism_indices[l]]) - (1/M)*sum(kernel_matrix[prototype_indices,criticism_indices[l]]))\n",
    "        #overall_losses[l] = np.abs((1/N)*sum(kernel_matrix[range(N), criticism_indices[l]]) - (1/M)*sum(kernel_matrix[true_proto_indices, criticism_indices[l]]))\n",
    "        losses[l] = np.abs((1/n)*np.inner(full_phi_sum, phis_for_criticisms[l,:]) - (1/M)*np.inner(proto_phi_sum, phis_for_criticisms[l,:]))\n",
    "        #overall_losses[l] = np.abs((1/N)*np.inner(full_stream_phi, phis_for_criticisms[l,:]) - (1/M)*np.inner(proto_phi_sum, phis_for_criticisms[l,:]))\n",
    "    \n",
    "    total_loss = sum(losses)\n",
    "    F = np.zeros(M+1)\n",
    "    for y in range(M+1):\n",
    "        #print criticism_kernel_matrix\n",
    "        mate = criticism_kernel_matrix[np.setdiff1d(range(M+1), y), :][:, np.setdiff1d(range(M+1), y)]\n",
    "        #print mate.shape\n",
    "        r = np.log(np.linalg.det(criticism_kernel_matrix[np.setdiff1d(range(M+1), y), :][:, np.setdiff1d(range(M+1), y)]))\n",
    "        F[y] = total_loss - losses[y] + r\n",
    "    #print(sum(overall_losses))\n",
    "    \n",
    "    idx_to_exclude = np.argmax(F)\n",
    "    #print F[idx_to_exclude]\n",
    "    criticism_indices = np.delete(criticism_indices, idx_to_exclude)\n",
    "    phis_for_criticisms = np.delete(phis_for_criticisms, idx_to_exclude, axis=0)\n",
    "    \n",
    "    #print(losses)\n",
    "    #min_loss_idx = np.argmin(losses)\n",
    "    #min_loss_idx = np.argmin(overall_losses)\n",
    "    #min_loss = losses[min_loss_idx]\n",
    "    #new_loss = losses[M]\n",
    "    #print(min_loss_idx)\n",
    "    #print(sum(overall_losses))\n",
    "    #print(sum(losses))\n",
    "\n",
    "    #if new_F > min_F:\n",
    "        #print(criticism_indices)\n",
    "        #criticism_indices = np.delete(criticism_indices, min_F_idx)\n",
    "        #phis_for_criticisms = np.delete(phis_for_criticisms, (min_F_idx), axis=0)\n",
    "        #print phis_for_criticisms.shape\n",
    "        \n",
    "    #else:\n",
    "        #criticism_indices = np.delete(criticism_indices, M)\n",
    "        #phis_for_criticisms = np.delete(phis_for_criticisms, (M), axis=0)\n",
    "        #print phis_for_criticisms.shape\n",
    "        \n",
    "    olm_return = [criticism_indices, phis_for_criticisms]\n",
    "    return olm_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "5000\n",
      "5500\n",
      "6000\n",
      "6500\n",
      "7000\n",
      "7500\n",
      "8000\n",
      "8500\n",
      "9000\n",
      "9500\n",
      "10000\n",
      "10500\n",
      "11000\n",
      "11500\n",
      "12000\n",
      "12500\n",
      "13000\n",
      "13500\n",
      "14000\n",
      "14500\n",
      "15000\n",
      "15500\n",
      "16000\n",
      "16500\n",
      "17000\n",
      "17500\n",
      "18000\n",
      "18500\n",
      "19000\n",
      "19500\n",
      "20000\n",
      "20500\n",
      "21000\n",
      "21500\n",
      "22000\n",
      "22500\n",
      "23000\n",
      "23500\n",
      "24000\n",
      "24500\n",
      "25000\n",
      "25500\n",
      "26000\n",
      "26500\n",
      "27000\n",
      "27500\n",
      "28000\n",
      "28500\n",
      "29000\n",
      "29500\n",
      "30000\n",
      "30500\n",
      "31000\n",
      "31500\n",
      "32000\n",
      "32500\n",
      "33000\n",
      "33500\n",
      "34000\n",
      "34500\n",
      "35000\n",
      "35500\n",
      "36000\n",
      "36500\n",
      "37000\n",
      "37500\n",
      "38000\n",
      "38500\n",
      "39000\n",
      "39500\n",
      "40000\n",
      "40500\n",
      "41000\n",
      "41500\n",
      "42000\n",
      "42500\n",
      "43000\n",
      "43500\n",
      "44000\n",
      "44500\n",
      "45000\n",
      "45500\n",
      "46000\n",
      "46500\n",
      "47000\n",
      "47500\n",
      "48000\n",
      "48500\n",
      "49000\n",
      "49500\n"
     ]
    }
   ],
   "source": [
    "#The below generates the function to calculate phi_hat, median_heuristic calcuates a value for gamma.\n",
    "gamma = rff.median_heuristic(stream[:M])\n",
    "phi = rff.RFF(500, 784, lengthscale=gamma) \n",
    "\n",
    "#kernel_matrix = compute_kernel(stream, N, gamma = 0.029)\n",
    "\n",
    "#Initialization:\n",
    "prototype_points = np.array(stream[:M])\n",
    "criticism_indices = range(M, 2*M)\n",
    "phis_for_criticisms = phi(np.array(stream[criticism_indices]))\n",
    "\n",
    "# Compute batch benchmark feature mean\n",
    "#full_stream_phi = phi(stream).mean(0)\n",
    "#full_stream_sq_norm = np.sum(full_stream_phi**2,0)\n",
    "\n",
    "subsampler = subsample.LinearSubsampler(prototype_points, phi, logging=True)\n",
    "full_stream_phi = sum(phi(stream))\n",
    "\n",
    "#for i in xrange(2*M, len(stream)):\n",
    "for i in range(2*M, N):\n",
    "    if np.mod(i, 500) == 1:\n",
    "        print i-1\n",
    "    subsampler.consider(stream[i])\n",
    "    prototype_indices = subsampler.which\n",
    "    is_accepted = subsampler.accepted[-1]\n",
    "    #print (i+1)*subsampler._full_mean_phi\n",
    "    #print subsampler.total_sum_test\n",
    "    \n",
    "    #prototype_indices = np.asarray([5520, 9233, 8410, 2765 ,5408, 7700 ,6251 ,3124, 3154 ,8664])\n",
    "    \n",
    "    if not is_accepted:\n",
    "        olm_return = online_loss_reg_max(new_phi = phi(stream[i]), phis_for_criticisms = phis_for_criticisms, \n",
    "                                         full_phi_sum = subsampler.total_sum_test, \n",
    "                                         proto_phi_sum = subsampler.sample_sum_test, \n",
    "                                         prototype_indices = prototype_indices, \n",
    "                                         criticism_indices = criticism_indices, n = i+1, \n",
    "                                         M = M, full_stream_phi = full_stream_phi, N = N,\n",
    "                                         stream = stream, gamma = 0.029)\n",
    "                                         #kernel_matrix = kernel_matrix)\n",
    "                                         #true_proto_indices = np.asarray([912, 755, 576, 919 ,962, 902 ,775 ,850 ,906, 948]))\n",
    "        criticism_indices = olm_return[0]\n",
    "        #print criticism_indices\n",
    "        #print criticism_indices\n",
    "        phis_for_criticisms = olm_return[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#resultImages = online_loss_reg_max(stream, gamma, new_phi, phis_for_criticisms, \n",
    "#                                   full_phi_sum, proto_phi_sum, prototype_indices, \n",
    "#                                   criticism_indices, n, M, full_stream_phi, N):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
