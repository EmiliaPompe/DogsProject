{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 2) => 100\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import time\n",
    "import onlinelossmax as olm\n",
    "sys.path.insert(0, './src')\n",
    "from scipy import stats\n",
    "import rff, subsample, herding\n",
    "from kernelgenerator import compute_kernel\n",
    "%matplotlib inline\n",
    "\n",
    "class SyntheticTarget(object):\n",
    "    \n",
    "    def __init__(self, dist_components=2, dim=2, seed=None):\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "        self.dist_weights = stats.dirichlet(np.ones(dist_components)*10).rvs().ravel()\n",
    "        dist_centers = np.random.uniform(-4, 4, size=(dist_components, dim))\n",
    "\n",
    "        def gen_cov():\n",
    "            assert(dim == 2)\n",
    "            rho = np.random.rand()-0.5\n",
    "            sd = np.random.rand(dim)*0.4+0.5\n",
    "            L = np.zeros((dim,dim))\n",
    "            L[0,0] = sd[0]**2\n",
    "            L[1,1] = sd[1]**2\n",
    "            L[0,1] = rho*sd.prod()\n",
    "            return np.dot(L, L.T)\n",
    "\n",
    "        dist_cov = np.array([gen_cov() for i in xrange(dist_components)])\n",
    "        self.dist_obj = np.array([stats.multivariate_normal(dist_centers[k], dist_cov[k]) for k in xrange(dist_components)], dtype=object)\n",
    "        self.dim = dim\n",
    "\n",
    "    def compute_pdf(self, x):\n",
    "        return np.sum(self.dist_weights[:,None,None] * np.array([d.pdf(x) for d in self.dist_obj]), 0)\n",
    "\n",
    "    def draw_sample(self, ns=1):\n",
    "        component = np.random.multinomial(ns, self.dist_weights) # np.argmin(np.random.rand() > self.dist_weights.cumsum())\n",
    "        offset = np.concatenate(([0], component.cumsum()[:-1]))\n",
    "        samples = np.empty((ns,self.dim))\n",
    "        for c, n in enumerate(component):\n",
    "            if n > 0:\n",
    "                samples[offset[c]:offset[c]+n] = self.dist_obj[c].rvs(n) \n",
    "        np.random.shuffle(samples)\n",
    "        return samples\n",
    "\n",
    "    def draw_sample_slow(self):\n",
    "        component = np.argmin(np.random.rand() > self.dist_weights.cumsum())\n",
    "        return self.dist_obj[component].rvs()\n",
    "\n",
    "    def plot(self):\n",
    "        delta = 0.1\n",
    "        boundary = 5\n",
    "        x = np.arange(-boundary, boundary, delta)\n",
    "        y = np.arange(-boundary, boundary, delta)\n",
    "        X, Y = np.meshgrid(x, y)\n",
    "        Z = np.log(self.compute_pdf(np.concatenate((X[:,:,None],Y[:,:,None]),2)))\n",
    "        plt.figure()\n",
    "        contour_levels = np.arange(-9,1)\n",
    "        CS = plt.contour(X, Y, Z, levels=contour_levels)\n",
    "        plt.set_cmap('Blues')\n",
    "        \n",
    "        \n",
    "# Change for different sampled target dist\n",
    "SEED = 666\n",
    "ex = SyntheticTarget(seed=SEED, dist_components=2)\n",
    "N = 10000\n",
    "M = 100\n",
    "stream = ex.draw_sample(N)\n",
    "#stream.shape finds the dimensions of the nparray stream\n",
    "print stream.shape, \"=>\", M\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division  #So that when we divide by integers we gegt a float. Eg so that 5/2 = 2.5 not 2.\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def online_loss_reg_max(stream, gamma, new_phi, phis_for_criticisms, full_phi_sum, proto_phi_sum, prototype_indices, criticism_indices, n, M, full_stream_phi, N):\n",
    "    #ONLINE_LOSS_MAX maximizes L(C) online.\n",
    "    #Here we are adding x_n and assume that stored_phis and full_phi_sum have been updated to include x_new: the new data point.\n",
    "    \n",
    "    criticism_indices = np.hstack((criticism_indices,n-1))\n",
    "    criticism_kernel_matrix = compute_kernel(stream[criticism_indices], M+1, gamma)\n",
    "    phis_for_criticisms = np.vstack((phis_for_criticisms,new_phi))\n",
    "    #print phis_for_criticisms.shape\n",
    "    \n",
    "    losses = np.zeros(M+1)\n",
    "    overall_losses = np.zeros(M+1)\n",
    "    for l in range(M+1):\n",
    "        #losses[l] = np.abs((1/n)*sum(kernel_matrix[range(n),criticism_indices[l]]) - (1/M)*sum(kernel_matrix[prototype_indices,criticism_indices[l]]))\n",
    "        #overall_losses[l] = np.abs((1/N)*sum(kernel_matrix[range(N), criticism_indices[l]]) - (1/M)*sum(kernel_matrix[true_proto_indices, criticism_indices[l]]))\n",
    "        losses[l] = np.abs((1/n)*np.inner(full_phi_sum, phis_for_criticisms[l,:]) - (1/M)*np.inner(proto_phi_sum, phis_for_criticisms[l,:]))\n",
    "        #overall_losses[l] = np.abs((1/N)*np.inner(full_stream_phi, phis_for_criticisms[l,:]) - (1/M)*np.inner(proto_phi_sum, phis_for_criticisms[l,:]))\n",
    "    \n",
    "    total_loss = sum(losses)\n",
    "    F = np.zeros(M+1)\n",
    "    for y in range(M+1):\n",
    "        #print criticism_kernel_matrix\n",
    "        mate = criticism_kernel_matrix[np.setdiff1d(range(M+1), y), :][:, np.setdiff1d(range(M+1), y)]\n",
    "        #print mate.shape\n",
    "        r = np.log(np.linalg.det(criticism_kernel_matrix[np.setdiff1d(range(M+1), y), :][:, np.setdiff1d(range(M+1), y)]))\n",
    "        F[y] = total_loss - losses[y] + r\n",
    "    #print(sum(overall_losses))\n",
    "    \n",
    "    idx_to_exclude = np.argmax(F)\n",
    "    #print F[idx_to_exclude]\n",
    "    criticism_indices = np.delete(criticism_indices, idx_to_exclude)\n",
    "    phis_for_criticisms = np.delete(phis_for_criticisms, idx_to_exclude, axis=0)\n",
    "    \n",
    "    #print(losses)\n",
    "    #min_loss_idx = np.argmin(losses)\n",
    "    #min_loss_idx = np.argmin(overall_losses)\n",
    "    #min_loss = losses[min_loss_idx]\n",
    "    #new_loss = losses[M]\n",
    "    #print(min_loss_idx)\n",
    "    #print(sum(overall_losses))\n",
    "    #print(sum(losses))\n",
    "\n",
    "    #if new_F > min_F:\n",
    "        #print(criticism_indices)\n",
    "        #criticism_indices = np.delete(criticism_indices, min_F_idx)\n",
    "        #phis_for_criticisms = np.delete(phis_for_criticisms, (min_F_idx), axis=0)\n",
    "        #print phis_for_criticisms.shape\n",
    "        \n",
    "    #else:\n",
    "        #criticism_indices = np.delete(criticism_indices, M)\n",
    "        #phis_for_criticisms = np.delete(phis_for_criticisms, (M), axis=0)\n",
    "        #print phis_for_criticisms.shape\n",
    "        \n",
    "    olm_return = [criticism_indices, phis_for_criticisms]\n",
    "    return olm_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n"
     ]
    }
   ],
   "source": [
    "#The below generates the function to calculate phi_hat, median_heuristic calcuates a value for gamma.\n",
    "gamma = rff.median_heuristic(stream[:M])\n",
    "phi = rff.RFF(500, 2, lengthscale=gamma) \n",
    "\n",
    "#kernel_matrix = compute_kernel(stream, N, gamma = 0.029)\n",
    "\n",
    "#Initialization:\n",
    "prototype_points = np.array(stream[:M])\n",
    "criticism_indices = range(M, 2*M)\n",
    "phis_for_criticisms = phi(np.array(stream[criticism_indices]))\n",
    "\n",
    "# Compute batch benchmark feature mean\n",
    "#full_stream_phi = phi(stream).mean(0)\n",
    "#full_stream_sq_norm = np.sum(full_stream_phi**2,0)\n",
    "\n",
    "subsampler = subsample.LinearSubsampler(prototype_points, phi, logging=True)\n",
    "full_stream_phi = sum(phi(stream))\n",
    "\n",
    "#for i in xrange(2*M, len(stream)):\n",
    "for i in range(2*M, N):\n",
    "    if np.mod(i, 500) == 1:\n",
    "        print i-1\n",
    "    subsampler.consider(stream[i])\n",
    "    prototype_indices = subsampler.which\n",
    "    is_accepted = subsampler.accepted[-1]\n",
    "    #print (i+1)*subsampler._full_mean_phi\n",
    "    #print subsampler.total_sum_test\n",
    "    \n",
    "    #prototype_indices = np.asarray([5520, 9233, 8410, 2765 ,5408, 7700 ,6251 ,3124, 3154 ,8664])\n",
    "    \n",
    "    if not is_accepted:\n",
    "        olm_return = online_loss_reg_max(new_phi = phi(stream[i]), phis_for_criticisms = phis_for_criticisms, \n",
    "                                         full_phi_sum = subsampler.total_sum_test, \n",
    "                                         proto_phi_sum = subsampler.sample_sum_test, \n",
    "                                         prototype_indices = prototype_indices, \n",
    "                                         criticism_indices = criticism_indices, n = i+1, \n",
    "                                         M = M, full_stream_phi = full_stream_phi, N = N,\n",
    "                                         stream = stream, gamma = 0.029)\n",
    "                                         #kernel_matrix = kernel_matrix)\n",
    "                                         #true_proto_indices = np.asarray([912, 755, 576, 919 ,962, 902 ,775 ,850 ,906, 948]))\n",
    "        criticism_indices = olm_return[0]\n",
    "        #print criticism_indices\n",
    "        #print criticism_indices\n",
    "        phis_for_criticisms = olm_return[1]\n",
    "        \n",
    "print criticism_indices\n",
    "print prototype_indices\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = stream\n",
    "ex.plot()\n",
    "plt.plot(data[:N,0], data[:N,1], 'ro');\n",
    "plt.plot(data[prototype_indices,0], data[prototype_indices,1], 'bo');\n",
    "plt.plot(data[criticism_indices,0], data[criticism_indices,1], 'go');\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Online Criticism, Regularization (N =10000, M=100)')\n",
    "plt.axis([-5, 6, -6, 0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
